# Red Team Example: Garak Scanner on LLM Sandbox

This directory contains a setup to run **Garak**, an LLM vulnerability scanner, against a local LLM sandbox.

## Prerequisites

- **Podman** (or Docker) installed and running.
- **Python 3.12** installed.
- **Make** installed.
- **uv** installed.

## Usage

The `Makefile` provides convenient commands to manage the sandbox and run the scan.

### 1. Setup the Environment

To build and start the local LLM sandbox container:

```bash
make setup
```

### 2. Run Garak Scan

To execute a Garak scan:

```bash
make scan
```

This runs `garak` configured to talk to the local mock API at `http://localhost:8000/v1`.
By default, it runs the `encoding.InjectAscii85` probe as a demonstration. You can modify the `Makefile` to run different probes or full scans.

### 3. Cleanup

To stop and remove the sandbox container:

```bash
make stop
```

### Run All Steps

To run the entire flow (setup, scan, and cleanup) in sequence:

```bash
make all
```

## OWASP Top 10 Coverage

The Garak configuration (`config/garak.yaml`) has been tuned to include probes that map to the [OWASP Top 10 for LLM Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/).


| OWASP Top 10 Vulnerability | Garak Probe(s) | Description |
| :--- | :--- | :--- |
| **LLM01: Prompt Injection** | `ansiescape`, `continuation`, `dan`, `doctor`, `dra`, `encoding`, `fitd`, `goodside`, `latentinjection`, `phrasing`, `promptinject`, `sata` | Tests for direct injection, jailbreaks, and encoding obfuscation. |
| **LLM02: Insecure Output Handling** | `ansiescape`, `av_spam_scanning`, `exploitation`, `fitd`, `packagehallucination`, `web_injection` | Checks for XSS, RCE, and other output handling vulnerabilities. |
| **LLM04: Model Denial of Service** | `divergence` | Tests for resource exhaustion via divergence. |
| **LLM05: Supply Chain Vulnerabilities** | `ansiescape`, `fitd`, `glitch`, `goodside` | Checks for vulnerabilities in third-party components or data. |
| **LLM06: Sensitive Information Disclosure** | `divergence`, `donotanswer`, `exploitation`, `grandma`, `leakreplay`, `web_injection` | Checks for leakage of PII or sensitive data. |
| **LLM09: Overreliance** | `donotanswer`, `goodside`, `misleading`, `packagehallucination`, `snowball` | Tests for hallucination and false information. |
| **LLM10: Model Theft** | `divergence`, `leakreplay`, `topic` | Tests for model extraction or theft. |

> [!NOTE]
> Probes that are not text-based (e.g. `visual_jailbreak` which uses images, or `fileformats` which inspects files) have been excluded from this configuration as the current scope is focused on text-only interactions.
